{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import bigrams\n",
    "\n",
    "# Download Stopwords\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = TweetTokenizer()\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# Hyper parameters\n",
    "profile = False\n",
    "\n",
    "# Parameters for PART D\n",
    "use_stop_words = True\n",
    "use_stemming = False\n",
    "use_doc_freq = True\n",
    "\n",
    "# Parameters for PART E\n",
    "use_bigrams = True\n",
    "if use_bigrams:\n",
    "    punctuation = ['.', ',', ':', '(', ')', '?', '<', '?', '>', '\\'', '\\\"']\n",
    "    stop_words.extend(punctuation)\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "with open('ass2_data/train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "        \n",
    "with open('ass2_data/test.json') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "word_index = 0\n",
    "m = len(train_data)\n",
    "# m = 100000\n",
    "\n",
    "def get_vocab():\n",
    "    global word_index\n",
    "    for example in train_data:\n",
    "        # Add single words in vocab\n",
    "        review = []\n",
    "        # print (example[\"text\"])\n",
    "        for word in tokenizer.tokenize(example[\"text\"]):\n",
    "            word = word.lower()\n",
    "            word = word.replace('\\n', '')\n",
    "            if use_stop_words and word in stop_words:\n",
    "                continue\n",
    "            review.append(word)\n",
    "            if use_stemming:\n",
    "                word = ps.stem(word)\n",
    "            if word not in vocab:\n",
    "                vocab[word] = (word_index, 1)\n",
    "                word_index += 1\n",
    "            else:\n",
    "                vocab[word] = (vocab[word][0], vocab[word][1]+1)\n",
    "                \n",
    "        # Add bigrams in vocab\n",
    "        if use_bigrams:\n",
    "            bigrams = list(nltk.bigrams(review))\n",
    "            # print (bigrams)\n",
    "            for bigram in bigrams:\n",
    "                insert_bigram = bigram[0] + \"_\" + bigram[1] \n",
    "                if insert_bigram not in vocab:\n",
    "                    vocab[insert_bigram] = (word_index, 1)\n",
    "                    word_index += 1\n",
    "                else:\n",
    "                    vocab[insert_bigram] = (vocab[insert_bigram][0], vocab[insert_bigram][1]+1)    \n",
    "                    \n",
    "        if word_index%10000 == 0:\n",
    "            print (\"Added {0} words in dictionary\".format(word_index))\n",
    "        \n",
    "    vocab_text = open('ass2_data/vocab.txt', 'w')\n",
    "    for k, v in vocab.items():\n",
    "        vocab_text.write(\"{0} {1}\\n\".format(k, v[1]))\n",
    "    vocab_text.close()\n",
    "    print (len(vocab.keys()))\n",
    "\n",
    "try:\n",
    "    with open('ass2_data/vocab.txt') as f:\n",
    "        temp_vocab = f.readlines()\n",
    "        for word_tuple in temp_vocab:\n",
    "            word_tuple = word_tuple.rstrip()\n",
    "            split_index = word_tuple.rindex(' ')\n",
    "            word = word_tuple[0:split_index]\n",
    "            occurence = int(word_tuple[split_index+1:])\n",
    "            # print (\"{0} {1}\".format(word, occurence))\n",
    "            vocab[word] = (word_index, occurence)\n",
    "            word_index += 1\n",
    "        f.close()\n",
    "    print (len(vocab))\n",
    "except FileNotFoundError:\n",
    "    if profile:\n",
    "        %prun get_vocab()\n",
    "    else:\n",
    "        get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the algorithm\n",
    "V = len(vocab.keys())\n",
    "\n",
    "thetas = [1] * (V*5)\n",
    "phis = [0] * 5\n",
    "lengths = [V] * 5\n",
    "counts = [0] * 5\n",
    "\n",
    "# Apply Naive Bayes algorithm with simplified multinomial assumption \n",
    "# over the jth index word of ith document \n",
    "def evaluate_thetas():\n",
    "    for example in train_data[0:m]:\n",
    "        stars = int(example[\"stars\"])\n",
    "        counts[stars-1] += 1\n",
    "        lengths[stars-1] += len(example[\"text\"])\n",
    "        review = []\n",
    "        for word in tokenizer.tokenize(example[\"text\"]):\n",
    "            word = word.lower()\n",
    "            word = word.replace('\\n', '')\n",
    "            if use_stop_words and word in stop_words:\n",
    "                continue\n",
    "            review.append(word)\n",
    "            if use_stemming:\n",
    "                word = ps.stem(word)\n",
    "            t = vocab[word]\n",
    "            # print (\"Found word {0} at {1}\".format(word, t))\n",
    "            if ((not use_doc_freq) or (use_doc_freq and t[1] >= 2 and t[1] <= m/2)):\n",
    "                thetas[(stars-1)*V + t[0]] += 1\n",
    "        if use_bigrams:\n",
    "            bigrams = list(nltk.bigrams(review))\n",
    "            lengths[stars-1] += len(bigrams)\n",
    "            for bigram in bigrams:\n",
    "                final_bigram = bigram[0]+'_'+bigram[1]\n",
    "                t = vocab[final_bigram]\n",
    "                if ((not use_doc_freq) or (use_doc_freq and t[1] >= 2 and t[1] <= m/2)):\n",
    "                    thetas[(stars-1)*V + t[0]] += 1\n",
    "                    # print (bigram[0]+'_'+bigram[1], k)\n",
    "            \n",
    "if profile:\n",
    "    %prun evaluate_thetas()\n",
    "else:\n",
    "    evaluate_thetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final evaluations for the parameters\n",
    "log_thetas = [0] * (V*5)\n",
    "for k in range(V*5):\n",
    "    star_index = int(k/V)\n",
    "    log_thetas[k] = math.log(thetas[k]/lengths[star_index])\n",
    "    \n",
    "for c in range(5):\n",
    "    phis[c] = counts[c]/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m = len(test_data)\n",
    "# test_m = 5000\n",
    "confusion_matrix = np.zeros(shape=(5,5))\n",
    "\n",
    "def get_accuracy():\n",
    "    # Get the accuracy of the trained model over the training data and test data for PART A\n",
    "    accuracy = 0\n",
    "    for example in test_data[0:test_m]:\n",
    "        max_ll = 0\n",
    "        max_ll_stars = 0\n",
    "        for y in range(1,6):\n",
    "            ll = math.log(phis[y-1])\n",
    "            review = []\n",
    "            for word in tokenizer.tokenize(example[\"text\"]):\n",
    "                word = word.lower()\n",
    "                word = word.replace('\\n', '')\n",
    "                if use_stop_words and word in stop_words:\n",
    "                    continue\n",
    "                review.append(word)\n",
    "                if use_stemming:\n",
    "                    word = ps.stem(word)\n",
    "                try:\n",
    "                    t = vocab[word]\n",
    "                    # print (\"Found word {0} at {1}\".format(word, t))\n",
    "                    if ((not use_doc_freq) or (use_doc_freq and t[1] >= 2 and t[1] <= m/2)):\n",
    "                        ll += log_thetas[V*(y-1) + t[0]]\n",
    "                except:\n",
    "                    ll += math.log(1/V)\n",
    "            if use_bigrams:\n",
    "                bigrams = list(nltk.bigrams(review))\n",
    "                for bigram in bigrams:\n",
    "                    try:\n",
    "                        t = vocab[bigram[0]+'_'+bigram[1]]\n",
    "                        if ((not use_doc_freq) or (use_doc_freq and t[1] >= 2 and t[1] <= m/2)):\n",
    "                            ll += log_thetas[V*(y-1) + t[0]]\n",
    "                    except:\n",
    "                        ll += math.log(1/V)\n",
    "            # print (\"Likelihood: {0} for {1} stars\".format(ll, y))\n",
    "            if (max_ll == 0 or max_ll < ll):\n",
    "                max_ll = ll\n",
    "                max_ll_stars = y\n",
    "\n",
    "        # Calculated the max likelihood stars. Now check accuracy\n",
    "        # print (\"{0} {1}\".format(max_ll_stars, example[\"stars\"]))\n",
    "        if max_ll_stars == example[\"stars\"]:\n",
    "            accuracy += 1\n",
    "            \n",
    "        # Evaluate confusion matrix for PART C\n",
    "        confusion_matrix[max_ll_stars-1][int(example[\"stars\"])-1] += 1\n",
    "\n",
    "    print (accuracy/test_m * 100)\n",
    "    print (np.matrix(confusion_matrix))\n",
    "    \n",
    "if profile:\n",
    "    %prun get_accuracy()\n",
    "else:\n",
    "    get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Scores for PART F\n",
    "f1_scores = np.zeros(5)\n",
    "for i in range(5):\n",
    "    precision = confusion_matrix[i][i] / confusion_matrix.sum(axis=1)[i]\n",
    "    recall = confusion_matrix[i][i] / confusion_matrix.sum(axis=0)[i]\n",
    "    f1_scores[i] = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "print (f1_scores)\n",
    "print (f1_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART B\n",
    "accuracy_random = 0\n",
    "accuracy_majority = 0\n",
    "\n",
    "# Get accuracy of random prediction over test set\n",
    "for test_example in test_data:\n",
    "    stars = random.randint(1,5)\n",
    "    if stars == test_example[\"stars\"]:\n",
    "        accuracy_random += 1\n",
    "        \n",
    "# Get accuracy of majority prediction over test set\n",
    "stars_count = [0] * 5\n",
    "for example in train_data:\n",
    "    stars_count[int(example[\"stars\"])-1] += 1\n",
    "max_count_stars = stars_count.index(max(stars_count)) + 1\n",
    "\n",
    "for test_example in test_data:\n",
    "    if max_count_stars == test_example[\"stars\"]:\n",
    "        accuracy_majority += 1\n",
    "        \n",
    "print (accuracy_random/test_m * 100)\n",
    "print (accuracy_majority/test_m * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

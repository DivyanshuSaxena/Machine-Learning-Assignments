{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "train_raw = np.genfromtxt('./ass3_data/poker-hand-training-true.data', delimiter=',')\n",
    "test_raw = np.genfromtxt('./ass3_data/poker-hand-testing.data', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding for PART A\n",
    "train_df = pd.DataFrame(np.delete(train_raw, -1, 1))\n",
    "test_df = pd.DataFrame(np.delete(train_raw, -1, 1))\n",
    "train_outraw = train_raw[:, -1]\n",
    "test_outraw = test_raw[:, -1]\n",
    "\n",
    "train_data = pd.get_dummies(train_df, columns=train_df.columns).values\n",
    "test_data = pd.get_dummies(test_df, columns=train_df.columns).values\n",
    "train_output = pd.get_dummies(train_outraw).values\n",
    "test_output = pd.get_dummies(test_outraw).values\n",
    "\n",
    "num_features = len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_nn = False\n",
    "\n",
    "class Neural_Net:\n",
    "    batch_size = 0\n",
    "    num_inputs = 0\n",
    "    layers = []\n",
    "    num_outputs = 0\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    def __init__(self, b, i, h, o):\n",
    "        self.batch_size = b\n",
    "        self.num_inputs = i\n",
    "        self.num_outputs = o\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.1\n",
    "        \n",
    "        # Initialize Parameters for Hidden Layers\n",
    "        for layer in range(len(h)):\n",
    "            units = h[layer]\n",
    "            layer_weights = []\n",
    "            for unit in range(units):\n",
    "                unit_weights = []\n",
    "                unit_weights.append(random.random()-0.5) # Add bias weight\n",
    "                if layer == 0:\n",
    "                    for inp in range(self.num_inputs):\n",
    "                        unit_weights.append(random.random()-0.5)\n",
    "                else:\n",
    "                    for inp in range(h[layer-1]):\n",
    "                        unit_weights.append(random.random()-0.5)\n",
    "                layer_weights.append(np.array(unit_weights))\n",
    "            self.layers.append(np.array(layer_weights))\n",
    "        self_layers = np.array(self.layers)\n",
    "            \n",
    "        # Output Layer Perceptrons\n",
    "        layer_weights = []\n",
    "        for unit in range(self.num_outputs):\n",
    "            unit_weights = []\n",
    "            unit_weights.append(random.random()-0.5) # Add bias weight\n",
    "            for inp in range(h[-1]):\n",
    "                unit_weights.append(random.random()-0.5)\n",
    "            layer_weights.append(np.array(unit_weights))\n",
    "        self.layers.append(np.array(layer_weights))\n",
    "        print (\"Layer weights: {0}\".format(self.layers))\n",
    "        \n",
    "    def train(self, dataset, target):\n",
    "        batch = []\n",
    "        batch_target = []\n",
    "        num_epochs = 5000\n",
    "        prev_loss = 99999999\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_func = 0\n",
    "            for s_index in range(len(dataset)):\n",
    "                sample = dataset[s_index]\n",
    "                batch.append(sample)\n",
    "                batch_target.append(target[s_index])\n",
    "                if len(batch) < self.batch_size and s_index != len(dataset)-1:\n",
    "                    continue\n",
    "\n",
    "                if debug_nn:\n",
    "                    print (\"Starting with batch {0}\".format(batch))\n",
    "                netj_all = []\n",
    "                xjk_all = []\n",
    "                # Forward Propagation\n",
    "                for batch_sample in batch:\n",
    "                    xjk = copy.deepcopy(batch_sample)\n",
    "                    # Hidden Layers Computation\n",
    "                    for l_index in range(len(self.layers)):\n",
    "                        if debug_nn:\n",
    "                            print (\"Input to layer {0}: {1}\".format(l_index, xjk))\n",
    "                            layer_weights = self.layers[l_index]\n",
    "                            print (\"Layer {0} weights: {1}\".format(l_index, layer_weights))\n",
    "                        else:\n",
    "                            layer_weights = self.layers[l_index]\n",
    "                        if l_index < len(netj_all):\n",
    "                            xjk_complete = np.append(xjk, [1])\n",
    "                            xjk_all += (np.array([xjk_complete]))\n",
    "                            netj_all[l_index] += (np.dot(np.delete(layer_weights, -1, 1), xjk) + layer_weights[:, -1])\n",
    "                        else:\n",
    "                            xjk_complete = np.append(xjk, [1])\n",
    "                            xjk_all.append(np.array([xjk_complete]))\n",
    "                            netj_all.append(np.dot(np.delete(layer_weights, -1, 1), xjk) + layer_weights[:, -1])\n",
    "                        xjk = np.array(list(map(sigmoid, netj_all[-1])))\n",
    "                    output = xjk\n",
    "                    \n",
    "                if debug_nn:\n",
    "                    print (\"Forward Feed complete.\")\n",
    "                    print (\"Output: {0}\".format(output))\n",
    "                    print (\"Net J: {0}\".format(netj_all))\n",
    "                    print (\"X: {0}\".format(xjk_all))\n",
    "\n",
    "                # Back Propagation\n",
    "                updated_weights = copy.deepcopy(self.layers)\n",
    "                if debug_nn:\n",
    "                    print (\"Back Proping for output layer\")\n",
    "                    layer_weights = self.layers[-1]\n",
    "                    print (\"Working on layer weights: {0}\".format(layer_weights))\n",
    "                else:\n",
    "                    layer_weights = self.layers[-1]\n",
    "                xjk = xjk_all[-1]\n",
    "                del_netj = []\n",
    "                for sample_target in batch_target:\n",
    "                    loss_func += np.sum((sample_target-output)*(sample_target-output))\n",
    "                    if del_netj:\n",
    "                        del_netj += -1*(sample_target-output)*output*(1-output)\n",
    "                    else:\n",
    "                        del_netj = -1*(sample_target-output)*output*(1-output)\n",
    "                if debug_nn:\n",
    "                    print (\"Del netj at output layer: {0}\".format(del_netj))\n",
    "                    print (\"X at output layer: {0}\".format(xjk))\n",
    "                    del_j = np.matmul(np.array([del_netj]).T, xjk)\n",
    "                    print (\"Output derivative: {0}\".format(del_j))\n",
    "                    print (\"Subtracting {0} - {1}\".format(layer_weights, self.learning_rate*del_j/self.batch_size))\n",
    "                    updated_weights[-1] = np.subtract(layer_weights, self.learning_rate*del_j/self.batch_size)\n",
    "                    print (\"Updated weights for Output Layer: {0}\".format(updated_weights[-1]))\n",
    "                else:\n",
    "                    del_j = np.matmul(np.array([del_netj]).T, xjk)\n",
    "                    updated_weights[-1] = np.subtract(layer_weights, self.learning_rate*del_j/self.batch_size)\n",
    "                    \n",
    "                # Hidden Layers\n",
    "                for layer in reversed(range(len(self.layers[:-1]))):  \n",
    "                    if debug_nn:\n",
    "                        print (\"Back proping for layer {0}\".format(layer))\n",
    "                        layer_weights = self.layers[layer]\n",
    "                        print (\"Working on layer weights: {0}\".format(self.layers[layer]))\n",
    "                    else:\n",
    "                        layer_weights = self.layers[layer]\n",
    "                    netj = netj_all[layer]\n",
    "                    xjk = xjk_all[layer]\n",
    "                    signetj = np.array(list(map(sigmoid,netj)))\n",
    "                    del_lj = (np.delete(self.layers[layer+1], -1, 1) * signetj*(1-signetj))\n",
    "                    if debug_nn:\n",
    "                        print (\"Del lj at layer {0}: {1}\".format(layer, del_lj))\n",
    "                        del_netj = np.matmul(del_lj.T, del_netj)\n",
    "                        print (\"Del netj at layer {0}: {1}\".format(layer, del_netj))\n",
    "                        print (\"X at output layer: {0}\".format(xjk))\n",
    "                        del_j = np.matmul(np.array([del_netj]).T, xjk)\n",
    "                        print (\"Final derivative: {0}\".format(del_j))\n",
    "                    else:\n",
    "                        del_netj = np.matmul(del_lj.T, del_netj)\n",
    "                        del_j = np.matmul(np.array([del_netj]).T, xjk)\n",
    "                        \n",
    "                    updated_weights[layer] = np.subtract(layer_weights, self.learning_rate*del_j/self.batch_size)\n",
    "                self.layers = updated_weights\n",
    "\n",
    "                if debug_nn:\n",
    "                    print (\"Updated weights: {0}\".format(self.layers))\n",
    "                    print (\"-----------------------------------------\")\n",
    "                batch = []\n",
    "                batch_target = []\n",
    "            \n",
    "            if abs(prev_loss-loss_func) < 0.00005:\n",
    "                break\n",
    "            prev_loss = loss_func\n",
    "            if debug_nn:\n",
    "                print (loss_func)\n",
    "        print (\"Num Epochs: {0}\".format(epoch))\n",
    "        print (\"Final Weights: {0}\".format(self.layers))\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        xjk = copy.deepcopy(sample)\n",
    "        # Hidden Layers Computation\n",
    "        netj_all = []\n",
    "        for l_index in range(len(self.layers)):\n",
    "            print (\"Input to layer {0}: {1}\".format(l_index, xjk))\n",
    "            layer_weights = self.layers[l_index]\n",
    "            print (\"Layer {0} weights: {1}\".format(l_index, layer_weights))\n",
    "            if l_index < len(netj_all):\n",
    "                netj_all[l_index] += (np.dot(np.delete(layer_weights, -1, 1), xjk) + layer_weights[:, -1])\n",
    "            else:\n",
    "                netj_all.append(np.dot(np.delete(layer_weights, -1, 1), xjk) + layer_weights[:, -1])\n",
    "            xjk = np.array(list(map(sigmoid, netj_all[-1])))\n",
    "        output = xjk\n",
    "        print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Net(1, 3, [1], 2)\n",
    "# nn = Neural_Net(1, num_features, [5], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = [[0,0], [0,1], [1,0], [1,1]]\n",
    "# target = [[1,0], [0,1], [0,1], [0,1]]\n",
    "dataset = [[0,0,1], [0,1,1], [1,0,1], [1,1,1], [0,0,0], [1,0,0]]\n",
    "target = [[1,0], [0,1], [0,1], [0,1], [1,0], [1,0]]\n",
    "nn.train(dataset, target)\n",
    "# m = 1000\n",
    "# nn.train(train_data[0:m], train_output[0:m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict([1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(-0.67692362)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2,3])\n",
    "b = np.array([1,1])\n",
    "print (np.sum((a-b)*(a-b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

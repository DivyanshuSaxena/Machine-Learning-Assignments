{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "train_raw = np.genfromtxt('./ass3_data/poker-hand-training-true.data', delimiter=',')\n",
    "test_raw = np.genfromtxt('./ass3_data/poker-hand-testing.data', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding for PART A\n",
    "train_df = pd.DataFrame(np.delete(train_raw, -1, 1))\n",
    "test_df = pd.DataFrame(np.delete(test_raw, -1, 1))\n",
    "train_outraw = train_raw[:, -1]\n",
    "test_outraw = test_raw[:, -1]\n",
    "\n",
    "train_data = pd.get_dummies(train_df, columns=train_df.columns).values\n",
    "test_data = pd.get_dummies(test_df, columns=train_df.columns).values\n",
    "train_output = pd.get_dummies(train_outraw).values\n",
    "test_output = pd.get_dummies(test_outraw).values\n",
    "\n",
    "num_features = len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_nn = False\n",
    "\n",
    "class Neural_Net:\n",
    "    batch_size = 0\n",
    "    num_inputs = 0\n",
    "    layers = []\n",
    "    num_outputs = 0\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    def __init__(self, b, i, h, o):\n",
    "        self.batch_size = b\n",
    "        self.num_inputs = i\n",
    "        self.num_outputs = o\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.1\n",
    "        \n",
    "        # Initialize Parameters for Hidden Layers\n",
    "        for layer in range(len(h)):\n",
    "            units = h[layer]\n",
    "            layer_weights = []\n",
    "            for unit in range(units):\n",
    "                unit_weights = []\n",
    "                unit_weights.append(random.random()-0.5) # Add bias weight\n",
    "                if layer == 0:\n",
    "                    for inp in range(self.num_inputs):\n",
    "                        unit_weights.append(random.random()-0.5)\n",
    "                else:\n",
    "                    for inp in range(h[layer-1]):\n",
    "                        unit_weights.append(random.random()-0.5)\n",
    "                layer_weights.append(np.array(unit_weights))\n",
    "            self.layers.append(np.array(layer_weights))\n",
    "            \n",
    "        # Output Layer Perceptrons\n",
    "        layer_weights = []\n",
    "        for unit in range(self.num_outputs):\n",
    "            unit_weights = []\n",
    "            unit_weights.append(random.random()-0.5) # Add bias weight\n",
    "            for inp in range(h[-1]):\n",
    "                unit_weights.append(random.random()-0.5)\n",
    "            layer_weights.append(np.array(unit_weights))\n",
    "        self.layers.append(np.array(layer_weights))\n",
    "        if debug_nn:\n",
    "            print (\"Layer weights: {0}\".format(self.layers))\n",
    "        \n",
    "    def train(self, dataset, target):\n",
    "        batch = []\n",
    "        batch_target = []\n",
    "        max_epochs = 500\n",
    "        min_epochs = 100\n",
    "        prev_loss = 99999999\n",
    "        for epoch in range(max_epochs):\n",
    "            loss_func = 0\n",
    "            for s_index in range(len(dataset)):\n",
    "                sample = dataset[s_index]\n",
    "                batch.append(sample)\n",
    "                batch_target.append(target[s_index])\n",
    "                if len(batch) < self.batch_size and s_index != len(dataset)-1:\n",
    "                    continue\n",
    "                if debug_nn:\n",
    "                    print (\"Starting with batch {0}\".format(batch))\n",
    "                \n",
    "                netj_all = []\n",
    "                xjk_all = []\n",
    "                \n",
    "                # Forward Propagation\n",
    "                xjk = copy.deepcopy(batch)\n",
    "                for l_index in range(len(self.layers)):\n",
    "                    layer_weights = self.layers[l_index]\n",
    "                    if debug_nn:\n",
    "                        print (\"Input to layer {0}: {1}\".format(l_index, xjk))\n",
    "                        print (\"Layer {0} weights: {1}\".format(l_index, layer_weights))\n",
    "                    xjk_complete = np.insert(xjk, len(xjk[0]), 1, axis=1)\n",
    "                    xjk_all.append(xjk_complete)\n",
    "                    netj_layer = np.matmul(np.delete(layer_weights, -1, 1), np.array(xjk).T).T + np.array([layer_weights[:, -1]])\n",
    "                    netj_all.append(netj_layer)\n",
    "                    xjk = np.vectorize(sigmoid)(netj_all[-1])\n",
    "                output = xjk\n",
    "\n",
    "                if debug_nn:\n",
    "                    print (\"Forward Feed complete.\")\n",
    "                    print (\"Output: {0}\".format(output))\n",
    "                    print (\"Net J: {0}\".format(netj_all))\n",
    "                    print (\"X: {0}\".format(xjk_all))\n",
    "\n",
    "                # Calculate error\n",
    "                loss_func += np.sum((batch_target-output)*(batch_target-output))\n",
    "                \n",
    "                # Back Propagation\n",
    "                updated_weights = copy.deepcopy(self.layers)\n",
    "                layer_weights = self.layers[-1]\n",
    "                if debug_nn:\n",
    "                    print (\"Back Proping for output layer\")\n",
    "                    print (\"Working on layer weights: {0}\".format(layer_weights))\n",
    "                xjk = xjk_all[-1]\n",
    "                del_netj = -1*(batch_target-output)*output*(1-output)\n",
    "                if debug_nn:\n",
    "                    print (\"Del netj at output layer: {0}\".format(del_netj))\n",
    "                \n",
    "                del_j = []\n",
    "                for s_index in range(len(xjk)):\n",
    "                    xjk_sample = xjk[s_index]\n",
    "                    del_netj_sample = del_netj[s_index]\n",
    "                    del_j_sample = np.matmul(np.array([del_netj_sample]).T, np.array([xjk_sample]))\n",
    "                    if debug_nn:\n",
    "                        print (\"Sample X: {0}\".format(xjk_sample))\n",
    "                        print (\"Sample del j: {0}\".format(del_j_sample))\n",
    "                    if len(del_j) != 0:\n",
    "                        del_j = np.add(del_j, del_j_sample)\n",
    "                    else:\n",
    "                        del_j = del_j_sample\n",
    "                        \n",
    "                updated_weights[-1] = np.subtract(layer_weights, self.learning_rate*del_j/self.batch_size)\n",
    "                if debug_nn:\n",
    "                    print (\"Output derivative: {0}\".format(del_j))\n",
    "                    print (\"Updated weights for Output Layer: {0}\".format(updated_weights[-1]))\n",
    "                    \n",
    "                # Hidden Layers\n",
    "                for layer in reversed(range(len(self.layers[:-1]))):  \n",
    "                    layer_weights = self.layers[layer]\n",
    "                    if debug_nn:\n",
    "                        print (\"Back proping for layer {0}\".format(layer))\n",
    "                        print (\"Working on layer weights: {0}\".format(self.layers[layer]))\n",
    "                    netj = netj_all[layer]\n",
    "                    xjk = xjk_all[layer]\n",
    "                    del_j = []\n",
    "                    del_netj_new = []\n",
    "                    for s_index in range(len(xjk)):\n",
    "                        netj_sample = netj[s_index]\n",
    "                        xjk_sample = xjk[s_index]\n",
    "                        del_netj_sample = del_netj[s_index]\n",
    "                        signetj = np.vectorize(sigmoid)(netj_sample)\n",
    "                        del_lj_sample = (np.delete(self.layers[layer+1], -1, 1) * signetj*(1-signetj))\n",
    "                        del_netj_sample = np.matmul(del_lj_sample.T, del_netj_sample)\n",
    "                        del_j_sample = np.matmul(np.array([del_netj_sample]).T, np.array([xjk_sample]))\n",
    "                        if len(del_j) != 0:\n",
    "                            del_j = np.add(del_j, del_j_sample)\n",
    "                        else:\n",
    "                            del_j = del_j_sample\n",
    "                        del_netj_new.append(del_netj_sample)\n",
    "                    if debug_nn:\n",
    "                        print (\"Del netj for layer {0}: {1}\".format(layer, del_netj_new))\n",
    "                        print (\"Del j for layer {0}: {1}\".format(layer, del_j))\n",
    "                    del_netj = del_netj_new\n",
    "                        \n",
    "                    updated_weights[layer] = np.subtract(layer_weights, self.learning_rate*del_j/self.batch_size)\n",
    "                self.layers = updated_weights\n",
    "\n",
    "                if debug_nn:\n",
    "                    print (\"Updated weights: {0}\".format(self.layers))\n",
    "                    print (\"-----------------------------------------\")\n",
    "                batch = []\n",
    "                batch_target = []\n",
    "            \n",
    "            if epoch > min_epochs and abs(prev_loss-loss_func)/len(dataset) < 0.0001:\n",
    "                break\n",
    "            prev_loss = loss_func\n",
    "            # if debug_nn:\n",
    "            print (loss_func)\n",
    "        if debug_nn:\n",
    "            print (\"Final Weights: {0}\".format(self.layers))\n",
    "        print (\"Num Epochs: {0}\".format(epoch+1))\n",
    "        \n",
    "    def predict(self, sample):\n",
    "        xjk = copy.deepcopy(sample)\n",
    "        # Hidden Layers Computation\n",
    "        netj_all = []\n",
    "        for l_index in range(len(self.layers)):\n",
    "            layer_weights = self.layers[l_index]\n",
    "            if debug_nn:\n",
    "                print (\"Input to layer {0}: {1}\".format(l_index, xjk))\n",
    "                print (\"Layer {0} weights: {1}\".format(l_index, layer_weights))\n",
    "            if l_index < len(netj_all):\n",
    "                netj_all[l_index] += (np.dot(np.delete(layer_weights, -1, 1), xjk) + layer_weights[:, -1])\n",
    "            else:\n",
    "                netj_all.append(np.dot(np.delete(layer_weights, -1, 1), xjk) + layer_weights[:, -1])\n",
    "            xjk = np.array(list(map(sigmoid, netj_all[-1])))\n",
    "        output = xjk\n",
    "        if debug_nn:\n",
    "            print (output)\n",
    "        return output\n",
    "    \n",
    "    def get_accuracy(self, dataset, target):\n",
    "        accuracy = 0\n",
    "        confusion_matrix = np.zeros(shape=(self.num_outputs, self.num_outputs))\n",
    "        for s_index in range(len(dataset)):\n",
    "            sample = dataset[s_index]\n",
    "            output = self.predict(sample)\n",
    "            prediction = np.argmax(output)\n",
    "            if debug_nn:\n",
    "                print (\"Predicted: {0} Desired: {1}\".format(prediction, target[s_index]))\n",
    "            confusion_matrix[int(target[s_index])][prediction] += 1\n",
    "            if prediction == target[s_index]:\n",
    "                accuracy += 1\n",
    "        print (\"Accuracy: {0}\".format(accuracy/len(dataset)))\n",
    "        print (confusion_matrix)\n",
    "        return accuracy/len(dataset)*100, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = Neural_Net(1, 2, [1, 1], 2)\n",
    "dataset = [[0,0], [0,1], [1,0], [1,1]]\n",
    "target = [[1,0], [0,1], [0,1], [0,1]]\n",
    "# dataset = [[0,0,1], [0,1,1], [1,0,1], [1,1,1], [0,0,0], [1,0,0]]\n",
    "# target = [[1,0], [0,1], [0,1], [0,1], [1,0], [1,0]]\n",
    "nn1.train(dataset, target)\n",
    "# nn = Neural_Net(2, num_features, [25], 10)\n",
    "# nn.train(train_data[0:m], train_output[0:m])\n",
    "# nn.get_accuracy(train_data[0:m], train_outraw[0:m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1.predict([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(hidden_units, m, test_m):\n",
    "    train_accuracy = []\n",
    "    test_accuracy  = []\n",
    "    train_time     = []\n",
    "    train_cm       = []\n",
    "    test_cm        = []\n",
    "    for num_units in hidden_units:\n",
    "        nn = Neural_Net(2, num_features, num_units, 10)\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        nn.train(train_data[0:m], train_output[0:m])\n",
    "        end_time = time.time()\n",
    "        train_time.append(end_time-start_time)\n",
    "        # Accuracy\n",
    "        accuracy, cm = nn.get_accuracy(train_data[0:m], train_outraw[0:m])\n",
    "        train_accuracy.append(accuracy)\n",
    "        accuracy, cm = nn.get_accuracy(test_data[0:test_m], test_outraw[0:test_m])\n",
    "        train_cm.append(cm)\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_cm.append(cm)\n",
    "    return train_accuracy, test_accuracy, train_time, train_cm, test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = 100\n",
    "# test_m = 100\n",
    "m = len(train_data)\n",
    "test_m = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Plots for PART C\n",
    "hidden_units = [[5],[10],[15],[20],[25]]\n",
    "train_accuracy, test_accuracy, train_time, train_cm, test_cm = get_plots(hidden_units, m, test_m)\n",
    "\n",
    "xi = [i for i in range(0, len(hidden_units))]\n",
    "plt.plot(xi, train_time, 'b.-')\n",
    "plt.xticks(xi, hidden_units)\n",
    "plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "plt.ylabel(\"Train Time\")\n",
    "plt.savefig('./ass3_data/c_train_time.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(xi, train_accuracy, 'r.-', label='Train Accuracy')\n",
    "plt.plot(xi, test_accuracy, 'g.-', label='Test Accuracy')\n",
    "plt.xticks(xi, hidden_units)\n",
    "plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('./ass3_data/c_accuracy.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Plots for PART D\n",
    "hidden_units_d = [[5, 5],[10, 10],[15, 15],[20, 20],[25, 25]]\n",
    "train_accuracy_d, test_accuracy_d, train_time_d, train_cm_d, test_cm_d = get_plots(hidden_units_d, m, test_m)\n",
    "\n",
    "xi = [i for i in range(0, len(hidden_units_d))]\n",
    "plt.plot(xi, train_time_d, 'b.-')\n",
    "plt.xticks(xi, hidden_units_d)\n",
    "plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "plt.ylabel(\"Train Time\")\n",
    "plt.savefig('./ass3_data/d_train_time.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(xi, train_accuracy_d, 'r.-', label='Train Accuracy')\n",
    "plt.plot(xi, test_accuracy_d, 'g.-', label='Test Accuracy')\n",
    "plt.xticks(xi, hidden_units_d)\n",
    "plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('./ass3_data/d_accuracy.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_accuracy_d, test_accuracy_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

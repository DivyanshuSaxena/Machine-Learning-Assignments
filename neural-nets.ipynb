{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "train_raw = np.genfromtxt('./ass3_data/poker-hand-training-true.data', delimiter=',')\n",
    "test_raw = np.genfromtxt('./ass3_data/poker-hand-testing.data', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding for PART A\n",
    "train_df = pd.DataFrame(np.delete(train_raw, -1, 1))\n",
    "test_df = pd.DataFrame(np.delete(test_raw, -1, 1))\n",
    "train_outraw = train_raw[:, -1]\n",
    "test_outraw = test_raw[:, -1]\n",
    "\n",
    "train_data = pd.get_dummies(train_df, columns=train_df.columns).values\n",
    "test_data = pd.get_dummies(test_df, columns=train_df.columns).values\n",
    "train_output = pd.get_dummies(train_outraw).values\n",
    "test_output = pd.get_dummies(test_outraw).values\n",
    "\n",
    "num_features = len(train_data[0])\n",
    "adaptive = True\n",
    "single = False\n",
    "double = False\n",
    "relu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_nn = False\n",
    "tol = 0.0001\n",
    "\n",
    "class Neural_Net:\n",
    "    batch_size = 0\n",
    "    num_inputs = 0\n",
    "    layers = []\n",
    "    num_outputs = 0\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    def __init__(self, b, i, h, o):\n",
    "        self.batch_size = b\n",
    "        self.num_inputs = i\n",
    "        self.num_outputs = o\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.1\n",
    "        \n",
    "        # Initialize Parameters for Hidden Layers\n",
    "        for layer in range(len(h)):\n",
    "            units = h[layer]\n",
    "            layer_weights = []\n",
    "            for unit in range(units):\n",
    "                unit_weights = []\n",
    "                unit_weights.append(random.random()-0.5) # Add bias weight\n",
    "                if layer == 0:\n",
    "                    for inp in range(self.num_inputs):\n",
    "                        unit_weights.append(random.random()-0.5)\n",
    "                else:\n",
    "                    for inp in range(h[layer-1]):\n",
    "                        unit_weights.append(random.random()-0.5)\n",
    "                layer_weights.append(np.array(unit_weights))\n",
    "            self.layers.append(np.array(layer_weights))\n",
    "            \n",
    "        # Output Layer Perceptrons\n",
    "        layer_weights = []\n",
    "        for unit in range(self.num_outputs):\n",
    "            unit_weights = []\n",
    "            unit_weights.append(random.random()-0.5) # Add bias weight\n",
    "            for inp in range(h[-1]):\n",
    "                unit_weights.append(random.random()-0.5)\n",
    "            layer_weights.append(np.array(unit_weights))\n",
    "        self.layers.append(np.array(layer_weights))\n",
    "        if debug_nn:\n",
    "            print (\"Layer weights: {0}\".format(self.layers))\n",
    "        \n",
    "    def train(self, dataset, target):\n",
    "        batch = []\n",
    "        batch_target = []\n",
    "        max_epochs = 400\n",
    "        min_epochs = 100\n",
    "        prev_loss = 99999999\n",
    "        num_useless_epochs = 0\n",
    "        for epoch in range(max_epochs):\n",
    "            loss_func = 0\n",
    "            dataset_shuffle, target_shuffle = shuffle(dataset, target, random_state=epoch)\n",
    "            for s_index in range(len(dataset_shuffle)):\n",
    "                sample = dataset_shuffle[s_index]\n",
    "                batch.append(sample)\n",
    "                batch_target.append(target_shuffle[s_index])\n",
    "                if len(batch) < self.batch_size and s_index != len(dataset_shuffle)-1:\n",
    "                    continue\n",
    "                if debug_nn:\n",
    "                    print (\"Starting with batch {0}\".format(batch))\n",
    "                \n",
    "                xjk_all = []                                                    \n",
    "                # Forward Propagation\n",
    "                xjk = copy.deepcopy(batch)\n",
    "                for l_index in range(len(self.layers)):\n",
    "                    layer_weights = self.layers[l_index]\n",
    "                    xjk_complete = np.insert(xjk, len(xjk[0]), 1, axis=1)\n",
    "                    xjk_all.append(xjk_complete)\n",
    "                    o_j = np.matmul(np.delete(layer_weights, -1, 1), np.array(xjk).T).T + np.array([layer_weights[:, -1]])\n",
    "                    xjk = np.vectorize(sigmoid)(o_j)\n",
    "                output = xjk\n",
    "\n",
    "                if debug_nn:\n",
    "                    print (\"Forward Feed complete.\")\n",
    "\n",
    "                # Calculate error\n",
    "                loss_func += np.sum((batch_target-output)*(batch_target-output))\n",
    "                \n",
    "                # Back Propagation\n",
    "                updated_weights = copy.deepcopy(self.layers)\n",
    "                layer_weights = self.layers[-1]\n",
    "                xjk = xjk_all[-1]\n",
    "                # print (\"Dimension of xjk: {0}\".format(xjk.shape))\n",
    "                del_netj = (batch_target-output)*output*(1-output)\n",
    "                # print (\"Dimension of del_netj: {0}\".format(del_netj.shape))\n",
    "                del_j = np.matmul(del_netj.T, xjk)                        \n",
    "                updated_weights[-1] = np.add(layer_weights, self.learning_rate*del_j)\n",
    "                    \n",
    "                # Hidden Layers\n",
    "                for layer in reversed(range(len(self.layers[:-1]))):  \n",
    "                    layer_weights = self.layers[layer]\n",
    "                    xjk = xjk_all[layer]\n",
    "                    # print (\"Dimension of xjk: {0}\".format(xjk.shape))\n",
    "                    o_j = np.delete(xjk_all[layer+1], -1, 1)\n",
    "                    del_lj = np.matmul(del_netj, np.delete(self.layers[layer+1], -1, 1))\n",
    "                    # print (\"Dimension of del_lj: {0}\".format(del_lj.shape))\n",
    "                    del_netj = np.multiply(del_lj, (o_j*(1-o_j)))\n",
    "                    # print (\"Dimension of del_netj: {0}\".format(del_netj.shape))\n",
    "                    del_j = np.matmul(del_netj.T, xjk)\n",
    "                    # print (\"Dimension of del_j: {0}\".format(del_j.shape))                    \n",
    "                    updated_weights[layer] = np.add(layer_weights, self.learning_rate*del_j)\n",
    "                self.layers = updated_weights\n",
    "\n",
    "                if debug_nn:\n",
    "                    print (\"Updated weights: {0}\".format(self.layers))\n",
    "                    print (\"-----------------------------------------\")\n",
    "                batch = []\n",
    "                batch_target = []\n",
    "            \n",
    "            if abs(prev_loss-loss_func)/len(dataset_shuffle) < 0.00001:\n",
    "                if not adaptive:\n",
    "                    if epoch > min_epochs:\n",
    "                        break\n",
    "                else:\n",
    "                    num_useless_epochs += 1\n",
    "                    if num_useless_epochs == 2:\n",
    "                        self.learning_rate = self.learning_rate/5\n",
    "                    if self.learning_rate < 0.001:\n",
    "                        break\n",
    "            prev_loss = loss_func\n",
    "            if debug_nn:\n",
    "                print (\"Loss: {0}   Iteration: {1}\".format(loss_func, epoch))\n",
    "        if debug_nn:\n",
    "            print (\"Final Weights: {0}\".format(self.layers))\n",
    "        print (\"Num Epochs: {0}\".format(epoch+1))\n",
    "        \n",
    "    def predict(self, batch):\n",
    "        # Forward Propagation\n",
    "        xjk = copy.deepcopy(batch)\n",
    "        for l_index in range(len(self.layers)):\n",
    "            layer_weights = self.layers[l_index]\n",
    "            net_j = np.matmul(np.delete(layer_weights, -1, 1), np.array(xjk).T).T + np.array([layer_weights[:, -1]])\n",
    "            xjk = np.vectorize(sigmoid)(net_j)\n",
    "        output = xjk\n",
    "        return output\n",
    "    \n",
    "    def get_accuracy(self, dataset, target):\n",
    "        accuracy = 0\n",
    "        confusion_matrix = np.zeros(shape=(self.num_outputs, self.num_outputs))\n",
    "        output = self.predict(dataset)\n",
    "        prediction = np.argmax(output, axis=1)\n",
    "        for s_index in range(len(dataset)):\n",
    "            pred = prediction[s_index]\n",
    "            if debug_nn:\n",
    "                print (\"Predicted: {0} Desired: {1}\".format(pred, target[s_index]))\n",
    "            confusion_matrix[int(target[s_index])][pred] += 1\n",
    "            if pred == target[s_index]:\n",
    "                accuracy += 1\n",
    "        print (\"Accuracy: {0}\".format(accuracy/len(dataset)))\n",
    "        print (confusion_matrix)\n",
    "        return accuracy/len(dataset)*100, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots(hidden_units, m, test_m, name, color):\n",
    "    train_accuracy = []\n",
    "    test_accuracy  = []\n",
    "    train_time     = []\n",
    "    train_cm       = []\n",
    "    test_cm        = []\n",
    "    for num_units in hidden_units:\n",
    "        nn = Neural_Net(100, num_features, num_units, 10)\n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        nn.train(train_data[0:m], train_output[0:m])\n",
    "        end_time = time.time()\n",
    "        train_time.append(end_time-start_time)\n",
    "        # Accuracy\n",
    "        accuracy, cm = nn.get_accuracy(train_data[0:m], train_outraw[0:m])\n",
    "        train_accuracy.append(accuracy)\n",
    "        accuracy, cm = nn.get_accuracy(test_data[0:test_m], test_outraw[0:test_m])\n",
    "        train_cm.append(cm)\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_cm.append(cm)\n",
    "    \n",
    "    # Save figures\n",
    "    xi = [i for i in range(0, len(hidden_units))]\n",
    "    plt.figure()\n",
    "    plt.plot(xi, train_time, 'b.-')\n",
    "    plt.xticks(xi, hidden_units)\n",
    "    plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "    plt.ylabel(\"Train Time\")\n",
    "    plt.ylim(30,300)\n",
    "    plt.savefig('./ass3_data/{0}_train_time.png'.format(name))\n",
    "    \n",
    "    xi = [i for i in range(0, len(hidden_units))]\n",
    "    plt.figure()\n",
    "    plt.plot(xi, train_accuracy, 'r.-', label='Train Accuracy')\n",
    "    plt.plot(xi, test_accuracy, 'g.-', label='Test Accuracy')\n",
    "    plt.xticks(xi, hidden_units)\n",
    "    plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.ylim(50,100)\n",
    "    plt.savefig('./ass3_data/{0}_accuracy.png'.format(name))\n",
    "\n",
    "    # Plot Confusion Matrices\n",
    "    units = [5, 10, 15, 20, 25]\n",
    "    for cm_index in range(len(test_cm)):\n",
    "        cm = test_cm[cm_index]\n",
    "        plt.figure()\n",
    "        ax = sns.heatmap(cm, cmap=color)\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig('./ass3_data/{0}{1}.png'.format(name, units[cm_index]))\n",
    "\n",
    "    return train_accuracy, test_accuracy, train_time, train_cm, test_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = 100\n",
    "# test_m = 100\n",
    "m = len(train_data)\n",
    "test_m = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Plots for PART C, D\n",
    "hidden_units_single = [[5],[10],[15],[20],[25]]\n",
    "hidden_units_double = [[5,5],[10,10],[15,15],[20,20],[25,25]]\n",
    "\n",
    "if single:\n",
    "    get_plots(hidden_units_single, m, test_m, 'c', \"PuBuGn\")\n",
    "elif double:\n",
    "    get_plots(hidden_units_double, m, test_m, 'd', \"YlOrBr\")\n",
    "elif relu:\n",
    "    get_plots(hidden_units_single, m, test_m, 'fc', \"PuBuGn\")\n",
    "    get_plots(hidden_units_double, m, test_m, 'fd', \"YlOrBr\")\n",
    "else:\n",
    "    get_plots(hidden_units_single, m, test_m, 'ec', \"PuBuGn\")\n",
    "    get_plots(hidden_units_double, m, test_m, 'ed', \"YlOrBr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Plots for PART D\n",
    "hidden_units_d = [[5, 5],[10, 10],[15, 15],[20, 20],[25, 25]]\n",
    "train_accuracy_d, test_accuracy_d, train_time_d, train_cm_d, test_cm_d = get_plots(hidden_units_d, m, test_m)\n",
    "\n",
    "xi = [i for i in range(0, len(hidden_units_d))]\n",
    "plt.plot(xi, train_time_d, 'b.-')\n",
    "plt.xticks(xi, hidden_units_d)\n",
    "plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "plt.ylabel(\"Train Time\")\n",
    "plt.savefig('./ass3_data/d_train_time.png')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(xi, train_accuracy_d, 'r.-', label='Train Accuracy')\n",
    "plt.plot(xi, test_accuracy_d, 'g.-', label='Test Accuracy')\n",
    "plt.xticks(xi, hidden_units_d)\n",
    "plt.xlabel(\"Number of Perceptrons in Hidden Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('./ass3_data/d_accuracy.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn1 = Neural_Net(2, 3, [4, 5], 2)\n",
    "# dataset = [[0,0], [0,1], [1,0], [1,1]]\n",
    "# target = [[1,0], [0,1], [0,1], [0,1]]\n",
    "# dataset = [[0,0,1], [0,1,1], [1,0,1], [1,1,1], [0,0,0], [1,0,0]]\n",
    "# target = [[1,0], [0,1], [0,1], [0,1], [1,0], [1,0]]\n",
    "# nn1.train(dataset, target)\n",
    "# nn1.predict([1,0,1])\n",
    "# nn = Neural_Net(100, num_features, [20], 10)\n",
    "# nn.train(train_data[0:m], train_output[0:m])\n",
    "# nn.get_accuracy(train_data[0:m], train_outraw[0:m])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

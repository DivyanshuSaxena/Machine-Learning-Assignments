{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "train_raw = np.genfromtxt('./ass3_data/credit-cards.train.csv', delimiter=',', skip_header=2)\n",
    "test_raw = np.genfromtxt('./ass3_data/credit-cards.test.csv', delimiter=',', skip_header=2)\n",
    "val_raw = np.genfromtxt('./ass3_data/credit-cards.val.csv', delimiter=',', skip_header=2)\n",
    "\n",
    "reordered_attributes = [3,4,6,7,8,9,10,11,1,2,5,12,13,14,15,16,17,18,19,20,21,22,23]\n",
    "\n",
    "train_continuous = train_raw[:, [1,2,5,12,13,14,15,16,17,18,19,20,21,22,23]]\n",
    "test_continuous = test_raw[:, [1,2,5,12,13,14,15,16,17,18,19,20,21,22,23]]\n",
    "val_continuous = val_raw[:, [1,2,5,12,13,14,15,16,17,18,19,20,21,22,23]]\n",
    "train_category = train_raw[:, [3,4,6,7,8,9,10,11]]\n",
    "test_category = test_raw[:, [3,4,6,7,8,9,10,11]]\n",
    "val_category = val_raw[:, [3,4,6,7,8,9,10,11]]\n",
    "\n",
    "train_output = train_raw[:, -1]\n",
    "test_output = test_raw[:, -1]\n",
    "val_output = val_raw[:, -1]\n",
    "\n",
    "medians = np.apply_along_axis(np.median, 0, train_continuous)\n",
    "train_cont_data = np.zeros(len(train_raw))\n",
    "test_cont_data = np.zeros(len(test_raw))\n",
    "val_cont_data = np.zeros(len(val_raw))\n",
    "\n",
    "for i in range(len(train_continuous.T)):\n",
    "    column = train_continuous.T[i]\n",
    "    train_cont_data = np.column_stack((train_cont_data, mediate_data(column, medians[i])))\n",
    "\n",
    "for i in range(len(test_continuous.T)):\n",
    "    column = test_continuous.T[i]\n",
    "    test_cont_data = np.column_stack((test_cont_data, mediate_data(column, medians[i])))\n",
    "\n",
    "for i in range(len(val_continuous.T)):\n",
    "    column = val_continuous.T[i]\n",
    "    val_cont_data = np.column_stack((val_cont_data, mediate_data(column, medians[i])))\n",
    "    \n",
    "train_cont_data = np.delete(train_cont_data, 0, 1)\n",
    "test_cont_data = np.delete(test_cont_data, 0, 1)\n",
    "val_cont_data = np.delete(val_cont_data, 0, 1)\n",
    "\n",
    "train_data = np.concatenate((train_category, train_cont_data), axis=1)\n",
    "test_data = np.concatenate((test_category, test_cont_data), axis=1)\n",
    "val_data = np.concatenate((val_category, val_cont_data), axis=1)\n",
    "\n",
    "train_data_soph = np.concatenate((train_category, train_continuous), axis=1)\n",
    "test_data_soph = np.concatenate((test_category, test_continuous), axis=1)\n",
    "val_data_soph = np.concatenate((val_category, val_continuous), axis=1)\n",
    "\n",
    "attribute_values = {i: get_values(train_data.T[i]) for i in range(len(train_data[0]))}\n",
    "\n",
    "num_attributes = len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediate_data(array, median):\n",
    "    # median = np.median(array)\n",
    "    preprocess = lambda x : 0 if x < median else 1\n",
    "    return np.array([preprocess(xi) for xi in array])\n",
    "\n",
    "def get_values(array):\n",
    "    unique_values = np.unique(array)\n",
    "    if 7 in unique_values:\n",
    "        unique_values = np.array(range(-2, 10))\n",
    "    return unique_values\n",
    "\n",
    "def calculate_entropy(array):\n",
    "    entropy = 0\n",
    "    for x_tuple in array:\n",
    "        x_samples = sum(x_tuple)\n",
    "        x_entropy = 0\n",
    "        for y_samples in x_tuple:\n",
    "            x_entropy += (-y_samples*math.log(y_samples/x_samples))\n",
    "        entropy += x_samples*x_entropy\n",
    "    return entropy\n",
    "\n",
    "class Node:\n",
    "    children = []\n",
    "    attribute = -1\n",
    "    samples = []\n",
    "    outputs = []\n",
    "    medians = np.array([])\n",
    "    sample_class = 0\n",
    "    is_leaf = 1\n",
    "    \n",
    "    def __init__(self, dataset, output):\n",
    "        self.samples = np.array(dataset)\n",
    "        self.outputs = np.array(output)\n",
    "        self.children = []\n",
    "        self.medians = np.array([])\n",
    "        self.attribute = -1\n",
    "        self.sample_class = 0\n",
    "        self.is_leaf = 1\n",
    "    \n",
    "root = None\n",
    "sophisticated = False\n",
    "use_pp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(train_data)\n",
    "# m = 50\n",
    "epsilon = 0.05\n",
    "path_attr = []\n",
    "max_depth = 30\n",
    "times_split = [list() for i in range(num_attributes)]\n",
    "\n",
    "def grow_tree(curr, depth, parent, soph=False):\n",
    "    count_y = []\n",
    "    for X in attribute_values:\n",
    "        values = attribute_values[X]\n",
    "        temp_y = [] \n",
    "        for i in range(len(values)):\n",
    "            temp_y.append([epsilon, epsilon])\n",
    "        count_y.append(temp_y)\n",
    "        \n",
    "    # Preprocess at current node if sophisticated on\n",
    "    if soph:\n",
    "        curr_continuous = curr.samples[:, list(range(8,23))]\n",
    "        curr_category = curr.samples[:, list(range(8))]\n",
    "        curr_medians = np.apply_along_axis(np.median, 0, curr_continuous)\n",
    "        curr.medians = np.copy(curr_medians)\n",
    "        curr_cont_data = np.zeros(len(curr.samples))\n",
    "        for i in range(len(curr_continuous.T)):\n",
    "            column = curr_continuous.T[i]\n",
    "            curr_cont_data = np.column_stack((curr_cont_data, mediate_data(column, curr_medians[i])))\n",
    "        curr_cont_data = np.delete(curr_cont_data, 0, 1)\n",
    "        curr_train_data = np.concatenate((curr_category, curr_cont_data), axis=1)\n",
    "    else:\n",
    "        curr_train_data = curr.samples\n",
    "\n",
    "    # Compute y values\n",
    "    samples0 = samples1 = 0\n",
    "    for index in range(len(curr.samples)):\n",
    "        y = int(curr.outputs[index])\n",
    "        if y == 0:\n",
    "            samples0 += 1\n",
    "        else:\n",
    "            samples1 += 1\n",
    "        for feature_index in range(num_attributes):\n",
    "            attribute = int(curr_train_data[index][feature_index])\n",
    "            if feature_index in range(2,8):\n",
    "                attribute += 2\n",
    "            count_y[feature_index][attribute][y] += 1\n",
    "    \n",
    "    # print (\"For the current node, samples0: {0} and samples1: {1}\".format(samples0, samples1))\n",
    "    curr.sample_class = 1 if samples1 > samples0 else 0\n",
    "    # print (\"Giving class {0} for current node\".format(curr.sample_class)) # Debug    \n",
    "    \n",
    "    if depth < len(num_nodes):\n",
    "        num_nodes[depth] += 1\n",
    "    else:\n",
    "        num_nodes.append(1)\n",
    "    if (samples0 < 1 or samples1 < 1) or (parent[0] == samples0 and parent[1] == samples1):\n",
    "        # Found a leaf node\n",
    "        for attr in path_attr:\n",
    "            if path_attr.count(attr) > 1:\n",
    "                times_split[attr].append(path_attr.count(attr))\n",
    "        return 0\n",
    "    \n",
    "    # Compute entropies of each attribute and choose the best attribute\n",
    "    best_attribute = -1\n",
    "    best_entropy = 0\n",
    "    found_attribute = False\n",
    "    for X in attribute_values:\n",
    "        entropy = calculate_entropy(count_y[X])\n",
    "        if (entropy < best_entropy or best_attribute == -1):\n",
    "            if soph:\n",
    "                found_attribute = True\n",
    "                best_attribute = X\n",
    "                best_entropy = entropy\n",
    "            elif X not in path_attr:\n",
    "                found_attribute = True\n",
    "                best_attribute = X\n",
    "                best_entropy = entropy\n",
    "            \n",
    "    if not found_attribute:\n",
    "        # print (\"Exiting because no suitable attribute found\")\n",
    "        return 0\n",
    "\n",
    "    # print (\"Split at attribute {0}\".format(best_attribute, count_y[best_attribute])) # Debug\n",
    "    curr.attribute = best_attribute\n",
    "    path_attr.append(best_attribute)\n",
    "\n",
    "    # Make children and append them to the children array\n",
    "    children_dataset = []\n",
    "    children_output = []\n",
    "    for i in attribute_values[best_attribute]:\n",
    "        children_dataset.append([])\n",
    "        children_output.append([])\n",
    "    for index in range(len(curr.samples)):\n",
    "        attribute = int(curr_train_data[index][best_attribute])\n",
    "        if best_attribute in range(2,8):\n",
    "            attribute += 2\n",
    "        children_dataset[attribute].append(curr.samples[index])\n",
    "        children_output[attribute].append(curr.outputs[index])\n",
    "    for x in attribute_values[best_attribute]:\n",
    "        if best_attribute in range(2,8):\n",
    "            x += 2\n",
    "        child = Node(children_dataset[int(x)], children_output[int(x)])\n",
    "        curr.children.append(child)\n",
    "        \n",
    "    # Recursively grow tree on the children nodes\n",
    "    for child in curr.children:\n",
    "        if len(child.samples) > 0:\n",
    "            curr.is_leaf = 0\n",
    "            grow_tree(child, depth+1, [samples0, samples1], sophisticated)\n",
    "            \n",
    "    path_attr.remove(best_attribute)\n",
    "    # print (path_attr)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Accuracy functions\n",
    "def get_classification(sample, curr, max_depth, depth):\n",
    "    # Go to the child based on current best attribute\n",
    "    if depth == max_depth:\n",
    "        return curr.sample_class\n",
    "    if curr.is_leaf == 1:\n",
    "        return curr.sample_class\n",
    "    processed_sample = []\n",
    "    if sophisticated:\n",
    "        for attr_index in range(len(sample)):\n",
    "            if attr_index < 8:\n",
    "                processed_sample.append(sample[attr_index])\n",
    "            else:\n",
    "                attribute = sample[attr_index]\n",
    "                processed_attr = 0 if attribute < curr.medians[attr_index-8] else 1\n",
    "                processed_sample.append(processed_attr)\n",
    "        processed_sample = np.array(processed_sample)\n",
    "    else:\n",
    "        processed_sample = np.copy(sample)\n",
    "    # print (\"Starting for node with attribute: {0}, class: {1}, children: {3} and values: {2}\"\n",
    "    #       .format(curr.attribute, curr.sample_class, attribute_values[curr.attribute], len(curr.children))) # Debug\n",
    "    sample_attribute = processed_sample[curr.attribute]\n",
    "    if curr.attribute in range(2,8):\n",
    "        sample_attribute += 2\n",
    "    return get_classification(sample, curr.children[int(sample_attribute)], max_depth, depth+1)\n",
    "    \n",
    "def get_accuracy(dataset, dataset_output, max_depth):\n",
    "    accuracy = 0\n",
    "    for index in range(len(dataset)):\n",
    "        sample = dataset[index]\n",
    "        model_class = get_classification(sample, root, max_depth, 0)\n",
    "        if model_class == dataset_output[index]:\n",
    "            accuracy += 1\n",
    "    # print (accuracy/len(dataset)*100)\n",
    "    return (accuracy/len(dataset)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(curr):\n",
    "    temp_nodes = []\n",
    "    for child in curr.children:\n",
    "        temp_nodes.append(child)\n",
    "        temp_nodes.extend(get_nodes(child))\n",
    "    return temp_nodes\n",
    "\n",
    "def post_prune(nodes, validation, validation_output, max_depth):\n",
    "    for curr in nodes:\n",
    "        if curr.is_leaf == 0:\n",
    "            accuracy = get_accuracy(validation, validation_output, max_depth)\n",
    "            curr.is_leaf = 1\n",
    "            accuracy_pruned = get_accuracy(validation, validation_output, max_depth)\n",
    "            if accuracy_pruned > accuracy:\n",
    "                curr.is_leaf = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = [1]\n",
    "if sophisticated:\n",
    "    root = Node(train_data_soph[0:m], train_output[0:m])\n",
    "else:\n",
    "    root = Node(train_data[0:m], train_output[0:m])\n",
    "grow_tree(root, 0, [0, 0], sophisticated)\n",
    "\n",
    "# PART B\n",
    "if use_pp:\n",
    "    nodes = get_nodes(root)\n",
    "    post_prune(nodes, val_data, val_output, len(num_nodes))\n",
    "\n",
    "# PART C\n",
    "if sophisticated:\n",
    "    multisplit_attribute = []\n",
    "    for attr in range(num_attributes):\n",
    "        if len(times_split[attr]) != 0:\n",
    "            split_tuple = [0,0]\n",
    "            split_tuple[0] = reordered_attributes[attr]\n",
    "            split_tuple[1] = np.max(times_split[attr])\n",
    "            multisplit_attribute.append(split_tuple)\n",
    "    print (\"Multiple splitted attributes: {0}\".format(multisplit_attribute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(num_nodes)):\n",
    "    num_nodes[i] = num_nodes[i] + num_nodes[i-1]\n",
    "print (num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training Accuracy\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for depth in range(0, len(num_nodes)):\n",
    "    if not sophisticated:\n",
    "        train_accuracy.append(get_accuracy(train_data[0:m], train_output[0:m], depth))\n",
    "        test_accuracy.append(get_accuracy(test_data, test_output, depth))\n",
    "        val_accuracy.append(get_accuracy(val_data, val_output, depth))\n",
    "    else:\n",
    "        train_accuracy.append(get_accuracy(train_data_soph[0:m], train_output[0:m], depth))\n",
    "        test_accuracy.append(get_accuracy(test_data_soph, test_output, depth))\n",
    "        val_accuracy.append(get_accuracy(val_data_soph, val_output, depth))\n",
    "    \n",
    "# print (train_accuracy)\n",
    "# print (test_accuracy)\n",
    "# print (val_accuracy)\n",
    "\n",
    "# Plot graph for PART A, B and C\n",
    "plt.figure()\n",
    "plt.plot(num_nodes, train_accuracy, 'b-', label='Train Accuracy')\n",
    "plt.plot(num_nodes, test_accuracy, 'r-', label='Test Accuracy')\n",
    "plt.plot(num_nodes, val_accuracy, 'g-', label='Validation Accuracy')\n",
    "plt.xlabel('Number of Nodes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.ylim(70,100)\n",
    "if sophisticated:\n",
    "    plt.savefig('./ass3_data/dtree_accuracy_c.png')\n",
    "elif use_pp:\n",
    "    plt.savefig('./ass3_data/dtree_accuracy_b.png')\n",
    "else:\n",
    "    plt.savefig('./ass3_data/dtree_accuracy_a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn for PART D\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "tree.fit(train_data, train_output)\n",
    "\n",
    "min_split_tree = DecisionTreeClassifier(criterion='entropy', min_samples_split=800)\n",
    "min_split_tree.fit(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)\n",
    "\n",
    "min_leaf_tree = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=85)\n",
    "min_leaf_tree.fit(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)\n",
    "\n",
    "max_depth_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "max_depth_tree.fit(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)\n",
    "\n",
    "# print (tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "# print (min_split_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "# print (min_leaf_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "# print (max_depth_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "\n",
    "print (\"Train Set Accuracy: {0}\".format(min_leaf_tree.score(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)))\n",
    "print (\"Test Set Accuracy: {0}\".format(min_leaf_tree.score(test_raw[:, list(range(1, len(test_raw[0])-1))], test_output)))\n",
    "print (\"Validation Set Accuracy: {0}\".format(min_leaf_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encodings of datasets\n",
    "concat_category = np.concatenate((np.concatenate((train_category, test_category), axis=0), val_category), axis=0)\n",
    "concat_df = pd.DataFrame(concat_category)\n",
    "concat_onehot = pd.get_dummies(concat_df, columns=concat_df.columns)\n",
    "\n",
    "train_onehot_cat = concat_onehot[:len(train_category)]\n",
    "test_onehot_cat = concat_onehot[len(train_category):len(train_category)+len(test_category)]\n",
    "val_onehot_cat = concat_onehot[len(train_category)+len(test_category):]\n",
    "\n",
    "train_onehot_data = np.concatenate((train_onehot_cat, train_continuous), axis=1)\n",
    "test_onehot_data = np.concatenate((test_onehot_cat, test_continuous), axis=1)\n",
    "val_onehot_data = np.concatenate((val_onehot_cat, val_continuous), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracies for PART E\n",
    "onehot_tree = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=85)\n",
    "onehot_tree.fit(train_onehot_data, train_output)\n",
    "print (\"Train Set Accuracy: {0}\".format(onehot_tree.score(train_onehot_data, train_output)))\n",
    "print (\"Test Set Accuracy: {0}\".format(onehot_tree.score(test_onehot_data, test_output)))\n",
    "print (\"Validation Set Accuracy: {0}\".format(onehot_tree.score(val_onehot_data, val_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracies for PART F\n",
    "forest = RandomForestClassifier(n_estimators=400, criterion='entropy', max_features=2, bootstrap=True, random_state=5)\n",
    "forest.fit(train_onehot_data, train_output)\n",
    "print (\"Train Set Accuracy: {0}\".format(forest.score(train_onehot_data, train_output)))\n",
    "print (\"Test Set Accuracy: {0}\".format(forest.score(test_onehot_data, test_output)))\n",
    "print (\"Validation Set Accuracy: {0}\".format(forest.score(val_onehot_data, val_output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediate_data(array, median):\n",
    "    # median = np.median(array)\n",
    "    preprocess = lambda x : 0 if x < median else 1\n",
    "    return np.array([preprocess(xi) for xi in array])\n",
    "\n",
    "def get_values(array):\n",
    "    unique_values = np.unique(array)\n",
    "    if 7 in unique_values:\n",
    "        unique_values = np.array(range(-2, 10))\n",
    "    return unique_values\n",
    "\n",
    "def calculate_entropy(array):\n",
    "    entropy = 0\n",
    "    for x_tuple in array:\n",
    "        x_samples = sum(x_tuple)\n",
    "        x_entropy = 0\n",
    "        for y_samples in x_tuple:\n",
    "            x_entropy += (-y_samples*math.log(y_samples/x_samples))\n",
    "        entropy += x_samples*x_entropy\n",
    "    return entropy\n",
    "\n",
    "class Node:\n",
    "    children = []\n",
    "    attribute = -1\n",
    "    sample_indexes = []\n",
    "    sample_class = 0\n",
    "    is_leaf = 1\n",
    "    \n",
    "    def __init__(self, index_list):\n",
    "        self.sample_indexes = index_list\n",
    "        self.children = []\n",
    "        self.attribute = -1\n",
    "        self.sample_class = 0\n",
    "        self.is_leaf = 1\n",
    "    \n",
    "root = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "train_raw = np.genfromtxt('./ass3_data/credit-cards.train.csv', delimiter=',', skip_header=2)\n",
    "test_raw = np.genfromtxt('./ass3_data/credit-cards.test.csv', delimiter=',', skip_header=2)\n",
    "val_raw = np.genfromtxt('./ass3_data/credit-cards.val.csv', delimiter=',', skip_header=2)\n",
    "\n",
    "train_continuous = train_raw[:, [1,2,5,12,13,14,15,16,17,18,19,20,21,22,23]]\n",
    "test_continuous = test_raw[:, [1,2,5,12,13,14,15,16,17,18,19,20,21,22,23]]\n",
    "val_continuous = val_raw[:, [1,2,5,12,13,14,15,16,17,18,19,20,21,22,23]]\n",
    "train_category = train_raw[:, [3,4,6,7,8,9,10,11]]\n",
    "test_category = test_raw[:, [3,4,6,7,8,9,10,11]]\n",
    "val_category = val_raw[:, [3,4,6,7,8,9,10,11]]\n",
    "\n",
    "train_output = train_raw[:, -1]\n",
    "test_output = test_raw[:, -1]\n",
    "val_output = val_raw[:, -1]\n",
    "\n",
    "medians = np.apply_along_axis(np.median, 0, train_continuous)\n",
    "train_cont_data = np.zeros(len(train_raw))\n",
    "test_cont_data = np.zeros(len(test_raw))\n",
    "val_cont_data = np.zeros(len(val_raw))\n",
    "\n",
    "for i in range(len(train_continuous.T)):\n",
    "    column = train_continuous.T[i]\n",
    "    train_cont_data = np.column_stack((train_cont_data, mediate_data(column, medians[i])))\n",
    "\n",
    "for i in range(len(test_continuous.T)):\n",
    "    column = test_continuous.T[i]\n",
    "    test_cont_data = np.column_stack((test_cont_data, mediate_data(column, medians[i])))\n",
    "\n",
    "for i in range(len(val_continuous.T)):\n",
    "    column = val_continuous.T[i]\n",
    "    val_cont_data = np.column_stack((val_cont_data, mediate_data(column, medians[i])))\n",
    "    \n",
    "train_cont_data = np.delete(train_cont_data, 0, 1)\n",
    "test_cont_data = np.delete(test_cont_data, 0, 1)\n",
    "val_cont_data = np.delete(val_cont_data, 0, 1)\n",
    "\n",
    "train_data = np.concatenate((train_category, train_cont_data), axis=1)\n",
    "test_data = np.concatenate((test_category, test_cont_data), axis=1)\n",
    "val_data = np.concatenate((val_category, val_cont_data), axis=1)\n",
    "\n",
    "attribute_values = {i: get_values(train_data.T[i]) for i in range(len(train_data[0]))}\n",
    "\n",
    "num_attributes = len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A\n",
    "m = len(train_data)\n",
    "# m = 5000\n",
    "# root = Node(list(range(m)))\n",
    "epsilon = 0.05\n",
    "path_attr = []\n",
    "max_depth = 24\n",
    "num_nodes = [0] * max_depth\n",
    "\n",
    "def grow_tree(curr, depth):\n",
    "    # print (\"Starting for node with indexes: {0}\".format(curr.sample_indexes)) # Debug\n",
    "    count_y = []\n",
    "    for X in attribute_values:\n",
    "        values = attribute_values[X]\n",
    "        temp_y = [] \n",
    "        for i in range(len(values)):\n",
    "            temp_y.append([epsilon, epsilon])\n",
    "        count_y.append(temp_y)\n",
    "\n",
    "    # Compute y values\n",
    "    samples0 = samples1 = 0\n",
    "    for index in curr.sample_indexes:\n",
    "        y = int(train_output[index])\n",
    "        if y == 0:\n",
    "            samples0 += 1\n",
    "        else:\n",
    "            samples1 += 1\n",
    "        for feature_index in range(num_attributes):\n",
    "            attribute = int(train_data[index][feature_index])\n",
    "            if feature_index in range(2,8):\n",
    "                attribute += 2\n",
    "            count_y[feature_index][attribute][y] += 1\n",
    "    \n",
    "    # print (\"For the current node, samples0: {0} and samples1: {1}\".format(samples0, samples1))\n",
    "    curr.sample_class = 1 if samples1 > samples0 else 0\n",
    "    # print (\"Giving class {0} for current node\".format(curr.sample_class)) # Debug\n",
    "    if samples0 < 1 or samples1 < 1:\n",
    "        return 0\n",
    "\n",
    "    num_nodes[depth] += 1\n",
    "    \n",
    "    # Compute entropies of each attribute and choose the best attribute\n",
    "    best_attribute = -1\n",
    "    best_entropy = 0\n",
    "    found_attribute = False\n",
    "    for X in attribute_values:\n",
    "        entropy = calculate_entropy(count_y[X])\n",
    "        if (entropy < best_entropy or best_attribute == -1) and X not in path_attr:\n",
    "            found_attribute = True\n",
    "            best_attribute = X\n",
    "            best_entropy = entropy\n",
    "            \n",
    "    if not found_attribute:\n",
    "        # print (\"Exiting because no suitable attribute found\")\n",
    "        return 0\n",
    "\n",
    "    # print (\"Split at attribute {0}\".format(best_attribute, count_y[best_attribute])) # Debug\n",
    "    curr.attribute = best_attribute\n",
    "    path_attr.append(best_attribute)\n",
    "\n",
    "    # Make children and append them to the children array\n",
    "    children_indexes = []\n",
    "    for i in attribute_values[best_attribute]:\n",
    "        children_indexes.append([])\n",
    "    for index in curr.sample_indexes:\n",
    "        attribute = int(train_data[index][best_attribute])\n",
    "        if best_attribute in range(2,8):\n",
    "            attribute += 2\n",
    "        children_indexes[attribute].append(index)\n",
    "    for x in attribute_values[best_attribute]:\n",
    "        if best_attribute in range(2,8):\n",
    "            x += 2\n",
    "        child = Node(children_indexes[int(x)])\n",
    "        curr.children.append(child)\n",
    "        \n",
    "    # Recursively grow tree on the children nodes\n",
    "    for child in curr.children:\n",
    "        if len(child.sample_indexes) > 0:\n",
    "            curr.is_leaf = 0\n",
    "            grow_tree(child, depth+1)\n",
    "            \n",
    "    path_attr.remove(best_attribute)\n",
    "    # print (path_attr)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Node(list(range(m)))\n",
    "grow_tree(root, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, max_depth):\n",
    "    num_nodes[i] = num_nodes[i] + num_nodes[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Accuracy functions\n",
    "def get_classification(sample, curr, max_depth, depth):\n",
    "    # Go to the child based on current best attribute\n",
    "    if depth == max_depth:\n",
    "        return curr.sample_class\n",
    "    # print (\"Starting for node with attribute: {0}, class: {1}, children: {3} and values: {2}\"\n",
    "    #       .format(curr.attribute, curr.sample_class, attribute_values[curr.attribute], len(curr.children))) # Debug\n",
    "    if curr.is_leaf == 1:\n",
    "        return curr.sample_class\n",
    "    sample_attribute = sample[curr.attribute]\n",
    "    if curr.attribute in range(2,8):\n",
    "        sample_attribute += 2\n",
    "    return get_classification(sample, curr.children[int(sample_attribute)], max_depth, depth+1)\n",
    "    \n",
    "def get_accuracy(dataset, dataset_output, max_depth):\n",
    "    accuracy = 0\n",
    "    for index in range(len(dataset)):\n",
    "        sample = dataset[index]\n",
    "        model_class = get_classification(sample, root, max_depth, 0)\n",
    "        if model_class == dataset_output[index]:\n",
    "            accuracy += 1\n",
    "    # print (accuracy/len(dataset)*100)\n",
    "    return (accuracy/len(dataset)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training Accuracy\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for depth in range(1, max_depth+1):\n",
    "    train_accuracy.append(get_accuracy(train_data[0:m], train_output[0:m], depth))\n",
    "    test_accuracy.append(get_accuracy(test_data, test_output, depth))\n",
    "    val_accuracy.append(get_accuracy(val_data, val_output, depth))\n",
    "    \n",
    "# print (train_accuracy, test_accuracy, val_accuracy)\n",
    "\n",
    "# Plot graph for PART A\n",
    "plt.plot(num_nodes, train_accuracy, 'b.-', label='Train Accuracy')\n",
    "plt.plot(num_nodes, test_accuracy, 'r.-', label='Test Accuracy')\n",
    "plt.plot(num_nodes, val_accuracy, 'g.-', label='Validation Accuracy')\n",
    "plt.xlabel('Number of Nodes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn for PART D\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "tree.fit(train_data, train_output)\n",
    "\n",
    "min_split_tree = DecisionTreeClassifier(criterion='entropy', min_samples_split=800)\n",
    "min_split_tree.fit(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)\n",
    "\n",
    "min_leaf_tree = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=85)\n",
    "min_leaf_tree.fit(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)\n",
    "\n",
    "max_depth_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "max_depth_tree.fit(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)\n",
    "\n",
    "# print (tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "# print (min_split_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "# print (min_leaf_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "# print (max_depth_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output))\n",
    "\n",
    "print (\"Train Set Accuracy: {0}\".format(min_leaf_tree.score(train_raw[:, list(range(1, len(train_raw[0])-1))], train_output)))\n",
    "print (\"Test Set Accuracy: {0}\".format(min_leaf_tree.score(test_raw[:, list(range(1, len(test_raw[0])-1))], test_output)))\n",
    "print (\"Validation Set Accuracy: {0}\".format(min_leaf_tree.score(val_raw[:, list(range(1, len(val_raw[0])-1))], val_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encodings of datasets\n",
    "concat_category = np.concatenate((np.concatenate((train_category, test_category), axis=0), val_category), axis=0)\n",
    "concat_df = pd.DataFrame(concat_category)\n",
    "concat_onehot = pd.get_dummies(concat_df, columns=concat_df.columns)\n",
    "\n",
    "train_onehot_cat = concat_onehot[:len(train_category)]\n",
    "test_onehot_cat = concat_onehot[len(train_category):len(train_category)+len(test_category)]\n",
    "val_onehot_cat = concat_onehot[len(train_category)+len(test_category):]\n",
    "\n",
    "train_onehot_data = np.concatenate((train_onehot_cat, train_continuous), axis=1)\n",
    "test_onehot_data = np.concatenate((test_onehot_cat, test_continuous), axis=1)\n",
    "val_onehot_data = np.concatenate((val_onehot_cat, val_continuous), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracies for PART E\n",
    "onehot_tree = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=85)\n",
    "onehot_tree.fit(train_onehot_data, train_output)\n",
    "print (\"Train Set Accuracy: {0}\".format(onehot_tree.score(train_onehot_data, train_output)))\n",
    "print (\"Test Set Accuracy: {0}\".format(onehot_tree.score(test_onehot_data, test_output)))\n",
    "print (\"Validation Set Accuracy: {0}\".format(onehot_tree.score(val_onehot_data, val_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracies for PART F\n",
    "forest = RandomForestClassifier(n_estimators=400, criterion='entropy', max_features=2, bootstrap=True, random_state=5)\n",
    "forest.fit(train_onehot_data, train_output)\n",
    "print (\"Train Set Accuracy: {0}\".format(forest.score(train_onehot_data, train_output)))\n",
    "print (\"Test Set Accuracy: {0}\".format(forest.score(test_onehot_data, test_output)))\n",
    "print (\"Validation Set Accuracy: {0}\".format(forest.score(val_onehot_data, val_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
